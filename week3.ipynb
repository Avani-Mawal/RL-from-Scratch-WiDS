{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Week 3\n",
    "## Efficiently finding optimal policies in MABs\n",
    "\n",
    "In this assignment, we will work with Multi Armed Bandit environments, and try to find the best policies using different strategies to minimize the total regret.\n",
    "\n",
    "The aim of this exercise is to code agents capable of understanding the underlying probability distributions of the environment and finding the most optimal actions as early as possible.\n",
    "\n",
    "You can start this assignment during/after reading Grokking Ch-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary stuff\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# if you want to use envs from Gym, import it\n",
    "# import gym, gym_bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a simple **2-armed Bernoulli** bandit.\n",
    "\n",
    "If you want a cleaner code, you can implement Bandits using `class` in Python.\n",
    "\n",
    "We have included sample code for this in `bandits.py` which you can take/import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the underlying probability distribution\n",
    "\n",
    "probs = np.random.random(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our MDP is a function which takes an action and returns a reward\n",
    "\n",
    "def mab_2_env(action):\n",
    "    gen = np.random.random()\n",
    "\n",
    "    # for bernoulli bandits, the reward is 1 if the random number is less than the probability of success, else 0\n",
    "    return gen < probs[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make a template for testing different strategies.\n",
    "\n",
    "Feel free to modify this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strategy function takes in the environment function, number of actions, and a selector function\n",
    "# it also takes in the number of episodes to run the strategy for (higher episodes = more accurate Q values)\n",
    "\n",
    "def test_strategy(env, n_actions, selector, n_episodes = 1000):\n",
    "    \n",
    "    # initialize Q and N to 0s\n",
    "    Q = np.zeros(n_actions)\n",
    "    N = np.zeros(n_actions)\n",
    "\n",
    "    # loop for n_episodes\n",
    "    for e in tqdm(range(n_episodes)):\n",
    "        \n",
    "        # selector function takes in current Q and returns an action\n",
    "        # modify the selector function according to the strategy\n",
    "        action = selector(Q)\n",
    "\n",
    "        # get the reward from the environment\n",
    "        reward = env(action)\n",
    "\n",
    "        # update N and Q\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action])/N[action]\n",
    "\n",
    "    # return the best action\n",
    "    return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the simplest selector using pure-exploration strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selector returns a random action\n",
    "random_selector = lambda Q : np.random.randint(len(Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a28110af7d4f71bddb2ec8682a8e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_strategy(mab_2_env, 2, random_selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it returns the optimal action. Let's check if that's indeed true.\n",
    "\n",
    "We can do that by revealing the actual `probs` distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.80499935, 0.65065254])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, our pure exploration strategy does indeed return the optimal action for this Bernoulli bandit. \n",
    "\n",
    "You can try generating new bandits with different `probs` and try out the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all this in place, here's what you have to do -\n",
    "\n",
    "Recall that, the regret $\\mathcal{T}$ is given by,\n",
    "\n",
    "$$\\mathcal{T}=\\sum  _{e=1} ^{E} \\mathbb{E} \\left[ v_* - q_* \\left( A_e \\right) \\right]$$\n",
    "\n",
    "We can only calculate it when we have the $v_*$ and $q_*$ functions known beforehand. Since we are making the MDPs from scratch, that's not an issue for us right now.\n",
    "\n",
    "But remember, in real-life problems, these functions are not known. Hence we must be aware of multiple policy finding strategies and try the one which gives best results fastest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 0\n",
    "\n",
    "Implement the calculation of the total regret $\\mathcal{T}$ for your strategy.\n",
    "\n",
    "To do this, you will need to store the rewards obtained each episode. Modify the `strategy` function accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate total regret\n",
    "def regret(rewards, optimal_probs):\n",
    "    T = len(rewards)\n",
    "    optimal_rewards = np.max(optimal_probs)\n",
    "    total_regret = optimal_rewards * T - np.sum(rewards)\n",
    "    return total_regret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 1\n",
    "\n",
    "Now, let's implement some other selection strategies and compare their regret with the simple exploration strategy.\n",
    "\n",
    "Note that some of these strategies involve hyperparameter(s) which need to be manually adjusted. You have to play around with the values and see which one gives you best results.\n",
    "\n",
    "This is known as \"hyperparameter tuning\" and is quite commonly done while working with complex models (including neural networks). Personally, you should try out some natural values (including the ones given in the book) along with some extreme values where it is easy to manually verify the correctness of your strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon-greedy strategy\n",
    "# Already implemented for you coz I am nice\n",
    "\n",
    "def epsilon_greedy(Q, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(len(Q))\n",
    "    else:\n",
    "        return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponentially_decaying_epsilon_greedy(Q, epsilon, episode):\n",
    "    decayed_epsilon = epsilon * np.exp(-0.01 * episode)  # You can adjust the decay rate\n",
    "    if np.random.random() < decayed_epsilon:\n",
    "        return np.random.randint(len(Q))\n",
    "    else:\n",
    "        return np.argmax(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_strategy(Q, temperature):\n",
    "    probabilities = np.exp(Q / temperature) / np.sum(np.exp(Q / temperature))\n",
    "    action = np.random.choice(len(Q), p=probabilities)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb(Q, N, c):\n",
    "    ucb_values = Q + c * np.sqrt(np.log(np.sum(N)) / (N + 1e-6))\n",
    "    return np.argmax(ucb_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thompson_sampling(Q, N, alpha, beta):\n",
    "    sampled_values = np.random.beta(alpha + Q, beta + N - Q)\n",
    "    return np.argmax(sampled_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 2\n",
    "\n",
    "Run each strategy for 2-armed bandit environment and compare the total regrets.\n",
    "\n",
    "You can also try plotting the regret vs episode graph and check if it matches the expected result from Grokking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def run_strategy(env, n_actions, selector, n_episodes=1000):\n",
    "    Q = np.zeros(n_actions)\n",
    "    N = np.zeros(n_actions)\n",
    "    rewards = []\n",
    "    optimal_rewards = []\n",
    "\n",
    "    for _ in tqdm(range(n_episodes)):\n",
    "        action = selector(Q)\n",
    "        reward = env(action)\n",
    "        rewards.append(reward)\n",
    "        optimal_rewards.append(np.max(env(np.arange(n_actions))))\n",
    "\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action]) / N[action]\n",
    "\n",
    "    total_regret = regret(rewards, optimal_rewards)\n",
    "    return total_regret\n",
    "\n",
    "# 2-armed Bernoulli bandit environment\n",
    "np.random.seed(42)\n",
    "n_actions_2_arm = 2\n",
    "probs_2_arm = np.random.random(n_actions_2_arm)\n",
    "env_2_arm = lambda action: np.random.random() < probs_2_arm[action]\n",
    "\n",
    "# Run each strategy for the 2-armed Bernoulli bandit environment\n",
    "n_episodes_2_arm = 1000\n",
    "epsilon_2_arm = 0.1\n",
    "c_ucb_2_arm = 2\n",
    "alpha_thompson_2_arm = 1\n",
    "beta_thompson_2_arm = 1\n",
    "\n",
    "regrets_2_arm = {\n",
    "    \"Random\": run_strategy(env_2_arm, n_actions_2_arm, random_selector, n_episodes_2_arm),\n",
    "    \"Epsilon-Greedy\": run_strategy(env_2_arm, n_actions_2_arm, lambda Q: epsilon_greedy(Q, epsilon_2_arm), n_episodes_2_arm),\n",
    "    \"Decaying Epsilon-Greedy\": run_strategy(env_2_arm, n_actions_2_arm,\n",
    "                                            lambda Q: exponentially_decaying_epsilon_greedy(Q, epsilon_2_arm, _),\n",
    "                                            n_episodes_2_arm),\n",
    "    \"Softmax\": run_strategy(env_2_arm, n_actions_2_arm, lambda Q: softmax_strategy(Q, temperature=0.1), n_episodes_2_arm),\n",
    "    \"UCB\": run_strategy(env_2_arm, n_actions_2_arm, lambda Q: ucb(Q, np.ones(n_actions_2_arm), c_ucb_2_arm),\n",
    "                        n_episodes_2_arm),\n",
    "    \"Thompson Sampling\": run_strategy(env_2_arm, n_actions_2_arm,\n",
    "                                      lambda Q: thompson_sampling(Q, np.ones(n_actions_2_arm), alpha_thompson_2_arm,\n",
    "                                                                  beta_thompson_2_arm),\n",
    "                                      n_episodes_2_arm)\n",
    "}\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "for strategy, total_regret in regrets_2_arm.items():\n",
    "    plt.plot(np.cumsum(total_regret), label=strategy)\n",
    "\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Cumulative Regret\")\n",
    "plt.legend()\n",
    "plt.title(\"Regret vs Episode for Different Bandit Strategies (2-armed Bernoulli bandit)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 3\n",
    "\n",
    "The 2-armed bandits might be too simple for us to actually see substantial difference in the regret of these strategies. \n",
    "\n",
    "Let's now create a more complicated bandit environment and replicate our results on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now implement a 10-armed Gaussian bandit. \n",
    "\n",
    "As required, it will have possible actions and each action will generate a reward sampled from a Gaussian distribution.\n",
    "\n",
    "Hence, each \"arm\" will have a randomly generated $\\mu$ and $\\sigma$, and the rewards will be generated with probabilities following the $\\mathcal{N}(\\mu, \\sigma^2)$ distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 arm gaussian bandit\n",
    "\n",
    "# Generate the means for each arm\n",
    "means_10_arm = np.random.normal(size=n_actions_10_arm)\n",
    "\n",
    "# Generate the variance for each arm\n",
    "variances_10_arm = np.ones(n_actions_10_arm)  # You can adjust variances if needed\n",
    "\n",
    "# Our MDP is a function which takes an action and returns a reward\n",
    "def mab_10_env(action):\n",
    "    # For Gaussian bandits, the reward is generated from a normal distribution\n",
    "    reward = np.random.normal(loc=means_10_arm[action], scale=np.sqrt(variances_10_arm[action]))\n",
    "    return reward\n",
    "\n",
    "# Test the 10-armed Gaussian bandit environment\n",
    "for _ in range(10):\n",
    "    action = np.random.randint(n_actions_10_arm)\n",
    "    reward = mab_10_env(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo 4\n",
    "\n",
    "Test the different strategies on the 10-armed gaussian bandit and verify your results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
